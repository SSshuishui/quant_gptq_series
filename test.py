full:  {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 
'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 
'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 
'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 
'self_attn.rotary_emb': LlamaRotaryEmbedding(), 
'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 
'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 
'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False), 
'input_layernorm': LlamaRMSNorm((4096,), eps=1e-05), 
'post_attention_layernorm': LlamaRMSNorm((4096,), eps=1e-05)}